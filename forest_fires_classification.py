# -*- coding: utf-8 -*-
"""project_final_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dH0Y_heHscJrC8aKg1qlQCNkKSSZ1-cz

# EE559 -  Final Project
# Algerial Forest Fires
## Done by:
## Vibhav Hosahalli Venkataramaiah
## Bhumi Godiwala
"""

# Commented out IPython magic to ensure Python compatibility.
# import all libraries
import datetime as dt
import copy
import pandas as pd
import numpy as np
import seaborn as sns
import random
import matplotlib.pyplot as plt
# %matplotlib inline 

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report,f1_score, confusion_matrix
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import NearestCentroid
from sklearn import preprocessing
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold
import sklearn.svm
from sklearn import tree
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

# load train and test data
train = pd.read_csv('algerian_fires_train.csv')
test = pd.read_csv('algerian_fires_test.csv')

train.head()

test.head()

train.columns

train_data = train.iloc[:,:-1]
train_label = train.iloc[:,-1]
train_data.isnull().sum()

test_data = test.iloc[:,:-1]
test_label = test.iloc[:,-1]
test_data.isnull().sum()

"""## Feature Engineering """

# Compute 2 day stats for 4 given parameters
def compute_stats(train_augmented):
    for i in range(4,train_augmented.shape[0],2):
        # compute stats for temperature
        train_augmented.loc[i,'Temperature_2day_avg'] = np.mean(train_augmented.iloc[(i-4):(i),1])
        train_augmented.loc[i+1,'Temperature_2day_avg']=np.mean(train_augmented.iloc[(i-4):(i),1])
        train_augmented.loc[i,'Temperature_2day_max']=max(train_augmented.iloc[(i-4):(i),1])
        train_augmented.loc[i+1,'Temperature_2day_max']=max(train_augmented.iloc[(i-4):(i),1])
        train_augmented.loc[i,'Temperature_2day_min']=min(train_augmented.iloc[(i-4):(i),1])
        train_augmented.loc[i+1,'Temperature_2day_min']=min(train_augmented.iloc[(i-4):(i),1])
        train_augmented.loc[i,'Temperature_2day_median']=np.median(train_augmented.iloc[(i-4):(i),1])
        train_augmented.loc[i+1,'Temperature_2day_median']=np.median(train_augmented.iloc[(i-4):(i),1])
        # Compute Stats for RH
        train_augmented.loc[i,'RH_2day_avg'] = np.mean(train_augmented.iloc[(i-4):(i),2])
        train_augmented.loc[i+1,'RH_2day_avg']=np.mean(train_augmented.iloc[(i-4):(i),2])
        train_augmented.loc[i,'RH_2day_max']=max(train_augmented.iloc[(i-4):(i),2])
        train_augmented.loc[i+1,'RH_2day_max']=max(train_augmented.iloc[(i-4):(i),2])
        train_augmented.loc[i,'RH_2day_min']=min(train_augmented.iloc[(i-4):(i),2])
        train_augmented.loc[i+1,'RH_2day_min']=min(train_augmented.iloc[(i-4):(i),2])
        train_augmented.loc[i,'RH_2day_median']=np.median(train_augmented.iloc[(i-4):(i),2])
        train_augmented.loc[i+1,'RH_2day_median']=np.median(train_augmented.iloc[(i-4):(i),2])
        # Compute stats for Ws
        train_augmented.loc[i,'Ws_2day_avg'] = np.mean(train_augmented.iloc[(i-4):(i),3])
        train_augmented.loc[i+1,'Ws_2day_avg']=np.mean(train_augmented.iloc[(i-4):(i),3])
        train_augmented.loc[i,'Ws_2day_max']=max(train_augmented.iloc[(i-4):(i),3])
        train_augmented.loc[i+1,'Ws_2day_max']=max(train_augmented.iloc[(i-4):(i),3])
        train_augmented.loc[i,'Ws_2day_min']=min(train_augmented.iloc[(i-4):(i),3])
        train_augmented.loc[i+1,'Ws_2day_min']=min(train_augmented.iloc[(i-4):(i),3])
        train_augmented.loc[i,'Ws_2day_median']=np.median(train_augmented.iloc[(i-4):(i),3])
        train_augmented.loc[i+1,'Ws_2day_median']=np.median(train_augmented.iloc[(i-4):(i),3])
        # Compute stats for Rain
        train_augmented.loc[i,'Rain_2day_avg'] = np.mean(train_augmented.iloc[(i-4):(i),4])
        train_augmented.loc[i+1,'Rain_2day_avg']=np.mean(train_augmented.iloc[(i-4):(i),4])
        train_augmented.loc[i,'Rain_2day_max']=max(train_augmented.iloc[(i-4):(i),4])
        train_augmented.loc[i+1,'Rain_2day_max']=max(train_augmented.iloc[(i-4):(i),4])
        train_augmented.loc[i,'Rain_2day_min']=min(train_augmented.iloc[(i-4):(i),4])
        train_augmented.loc[i+1,'Rain_2day_min']=min(train_augmented.iloc[(i-4):(i),4])
        train_augmented.loc[i,'Rain_2day_median']=np.median(train_augmented.iloc[(i-4):(i),4])
        train_augmented.loc[i+1,'Rain_2day_median']=np.median(train_augmented.iloc[(i-4):(i),4])
        
    return train_augmented

train_all_features = compute_stats(copy.copy(train)).iloc[4:,:]

"""# Correlation Matrix"""

plt.figure(figsize=(50, 50))
sns.heatmap(train_all_features.corr(),annot=True,cmap='Blues',linewidths=.5)

train_all_features.describe()

# Looking at the correlation matrix we see that for the stats computed for each field, all 4 of these new features are highly
# correlated to each other, thus we take only 1 of each, say avg. Along with this we also see that features BUI,DC and DMC are
# highly correlated to each other, thus we only take one of these features

# thus now create a function to compute only the our required features
def gen_req_features(train_aug):
    for i in range(4,train_aug.shape[0],2):
        # compute stats for temperature
        train_aug.loc[i,'Temperature_2day_avg'] = np.mean(train_aug.iloc[(i-4):(i),1])
        train_aug.loc[i+1,'Temperature_2day_avg']=np.mean(train_aug.iloc[(i-4):(i),1])
        
        # Compute Stats for RH
        train_aug.loc[i,'RH_2day_avg'] = np.mean(train_aug.iloc[(i-4):(i),2])
        train_aug.loc[i+1,'RH_2day_avg']=np.mean(train_aug.iloc[(i-4):(i),2])
        
        # Compute stats for Ws
        train_aug.loc[i,'Ws_2day_avg'] = np.mean(train_aug.iloc[(i-4):(i),3])
        train_aug.loc[i+1,'Ws_2day_avg']=np.mean(train_aug.iloc[(i-4):(i),3])
        # Compute stats for Rain
        train_aug.loc[i,'Rain_2day_avg'] = np.mean(train_aug.iloc[(i-4):(i),4])
        train_aug.loc[i+1,'Rain_2day_avg']=np.mean(train_aug.iloc[(i-4):(i),4])
    return train_aug

# generate the train and test data with our new features
train_augmented = gen_req_features(copy.copy(train)).iloc[4:,:]
test_augmented = gen_req_features(copy.copy(test)).iloc[4:,:]

train_augmented.columns

"""## Data Formatting """

# These variable will hold all the original features
train_data = train_data.drop(columns=['Date','DMC'])
test_data = test_data.drop(columns=['Date','DMC'])

# These variables will hold the original plus the augmented features as well
train_data_augmented = train_augmented.drop(columns=['Date','Classes','DMC'])
train_label_augmented = train_augmented['Classes']
test_data_augmented = test_augmented.drop(columns=['Date','Classes','DMC'])
test_label_augmented = test_augmented['Classes']

"""## Score Metrics - Accuracy, F1 score and Confusion Matrix"""

# Function to calculate the required score metrics
def score_metrics(actual_labels,predicted_labels, show = True):
  
  #Accuracy
    sys_accuracy = accuracy_score(actual_labels,predicted_labels)

  #F1 score
    sys_f1_score = f1_score(actual_labels,predicted_labels)
    
  #Confusion Matrix
    sys_cf = confusion_matrix(actual_labels,predicted_labels)
    
    if(show):
        print("Accuracy of system is ", sys_accuracy)
        print("F1 score of system is ", sys_f1_score)
        print("Confusion Matrix of system is \n", sys_cf)
        sns.heatmap(sys_cf,annot = True)
        
    return sys_accuracy

"""## Test Accuracy Function"""

# Function to compute accuracy for test data
def model_test_acc(model,data,labels,show = True):
    test_data_np = data
    test_labels_np = labels
    
    test_pred = np.zeros([len(test_data_np)])
    for i in range(len(test_data_np)):
        test_pred[i] = model.predict([test_data_np[i,:]])
    if(show):
        print("For testing data:")
    return score_metrics(test_labels_np,test_pred,show)

"""## Bernoulli Trivial System"""

# Function to define trivial system
def trivial(training_labels, test_labels):
  
    train_labels_np = training_labels.to_numpy()
    test_labels_np = test_labels.to_numpy()
    trivial_sys_prob = np.mean(train_labels_np)
    print("Probability of fire is ", trivial_sys_prob)

  # Creating a Bernoulli RV with the calculated probability
    train_pred = np.zeros([len(train_labels_np)])
    for i in range(len(train_labels_np)):
        n = random.uniform(0,1)
        if(n>=trivial_sys_prob):
            train_pred[i] = 1
        else:
            train_pred[i] = 0
    test_pred = np.zeros([len(test_labels_np)])
    for i in range(len(test_labels_np)):
        n = random.uniform(0,1)
        if(n>=trivial_sys_prob):
            test_pred[i] = 1
        else:
            test_pred[i] = 0
    print("For training data:")
    score_metrics(train_labels_np,train_pred)
    print("For testing data:")
    score_metrics(test_labels_np,test_pred)

trivial(train_label,test_label)

"""## Nearest Means Classifier """

# Function to define and fit Nearest Means Classifier
def nms_func(data,labels,show=True):
    
    train_data_np = data
    train_labels_np = labels
    
    clf_nms = NearestCentroid()
    clf_nms.fit(train_data_np,train_labels_np)

    train_pred_nms = np.zeros([len(train_data_np)])
    for i in range(len(train_data_np)):
        train_pred_nms[i] = clf_nms.predict([train_data_np[i,:]])

    score_metrics(train_labels_np,train_pred_nms,show)
    return clf_nms # we return the trained model

# We first run the Nearest Means Classifier with all original features
nms_orig_features = nms_func(train_data.to_numpy(),train_label.to_numpy())

# Running across the test data
model_test_acc(nms_orig_features,test_data.to_numpy(),test_label.to_numpy())

# Running across the augmented feature space
nms_aug_features = nms_func(train_data_augmented.to_numpy(),train_label_augmented.to_numpy())

model_test_acc(nms_aug_features,test_data_augmented.to_numpy(),test_label_augmented.to_numpy())

"""# Feature Selection and Data Normalization"""

# Normalization with the orginal features
train_data_orig_normalize = copy.copy(train_data.to_numpy())
train_label_orig_np = copy.copy(train_label.to_numpy())

train_data_orig_normalize = preprocessing.normalize(train_data_orig_normalize,axis=0)

nms_orig_norm = nms_func(train_data_orig_normalize,train_label_orig_np)

test_data_orig_normalize = copy.copy(test_data.to_numpy())
test_label_orig_np = copy.copy(test_label.to_numpy())

test_data_orig_normalize = preprocessing.normalize(test_data_orig_normalize,axis=0)

model_test_acc(nms_orig_norm,test_data_orig_normalize,test_label_orig_np)

# Normalization with the orginal+augmented features
train_data_aug_normalize = copy.copy(train_data_augmented.to_numpy())
train_label_aug_np = copy.copy(train_label_augmented.to_numpy())

train_data_aug_normalize = preprocessing.normalize(train_data_aug_normalize,axis=0)

nms_aug_norm = nms_func(train_data_aug_normalize,train_label_aug_np)

test_data_aug_normalize = copy.copy(test_data_augmented.to_numpy())
test_label_aug_np = copy.copy(test_label_augmented.to_numpy())

test_data_aug_normalize = preprocessing.normalize(test_data_aug_normalize,axis=0)

model_test_acc(nms_aug_norm,test_data_aug_normalize,test_label_aug_np)

"""# Dimensionality Reduction by PCA"""

# We select the best number of components by Cross-validation
kf = KFold(n_splits=7)
best_val_acc = 0
n_components = [3,4,5,6,7,8,9]
for n in n_components:
    val_acc=0
    for train_index,val_index in kf.split(train_data_augmented):
        
        train_data_np = copy.copy(train_data_augmented.iloc[train_index,:].to_numpy())
        train_label_np = copy.copy(train_label_augmented.iloc[train_index].to_numpy())
        
        train_data_np = preprocessing.normalize(train_data_np,axis=0)
    
        val_data_np = copy.copy(train_data_augmented.iloc[val_index,:].to_numpy())
        val_label_np = copy.copy(train_label_augmented.iloc[val_index].to_numpy())
        
        val_data_np = preprocessing.normalize(val_data_np,axis=0)
        
        pca = PCA(n_components=n)
        
        train_data_pca_np = pca.fit_transform(train_data_np)
        val_data_pca_np = pca.transform(val_data_np)
        
        # train nms
        nms_pca_func = nms_func(train_data_pca_np,train_label_np,False)
        #get val acc
        acc = model_test_acc(nms_pca_func,val_data_pca_np,val_label_np,False)
        val_acc = val_acc + acc
    val_acc = val_acc/7
    print(f'For n = {n}, validation accuracy is {val_acc}')
    if(val_acc > best_val_acc):
        best_val_acc = val_acc
        n_best = n
print(f'Best number of components is:{n_best}')

train_data_np = copy.copy(train_data_augmented.to_numpy())
train_label_np = copy.copy(train_label_augmented.to_numpy())
        
train_data_np = preprocessing.normalize(train_data_np,axis=0)        

pca = PCA(n_components=4)
        
train_data_pca_np = pca.fit_transform(train_data_np)
        
# train nms
nms_pca_func = nms_func(train_data_pca_np,train_label_np)

test_data_np = copy.copy(test_data_augmented.to_numpy())
test_label_np = copy.copy(test_label_augmented.to_numpy())
        
test_data_np = preprocessing.normalize(test_data_np,axis=0)

test_data_pca_np = pca.transform(test_data_np)

model_test_acc(nms_pca_func,test_data_pca_np,test_label_np)

"""# Dimensionality Reduction by LDA"""

train_data_np = copy.copy(train_data_augmented.to_numpy())
train_label_np = copy.copy(train_label_augmented.to_numpy())
        
train_data_np = preprocessing.normalize(train_data_np,axis=0)

lda = LDA()

train_data_lda = lda.fit(train_data_np,train_label_np)

# near_mean_lda = nearest_means(train_data_lda,train_label_np)
train_pred_lda = np.zeros([len(train_data_np)])
for i in range(len(train_data_np)):
    train_pred_lda[i] = lda.predict([train_data_np[i,:]])

score_metrics(train_pred_lda,train_label_np)

test_data_np = copy.copy(test_data_augmented.to_numpy())
test_label_np = copy.copy(test_label_augmented.to_numpy())
        
test_data_np = preprocessing.normalize(test_data_np,axis=0)

test_data_lda_np = lda.transform(test_data_np)

model_test_acc(lda,test_data_np,test_label_np)

"""## SVM"""

# we start by applying the SVM on the original and augmented data
def svm_func(data,labels,k,show=True):
    train_data_np = data
    train_labels_np = labels
    
    svm_func = sklearn.svm.SVC(kernel=k)
    svm_func.fit(train_data_np,train_labels_np)

    train_pred_nms = np.zeros([len(train_data_np)])
    for i in range(len(train_data_np)):
        train_pred_nms[i] = svm_func.predict([train_data_np[i,:]])

    score_metrics(train_labels_np,train_pred_nms,show)
    return svm_func # we return the trained model

train_data_np = copy.copy(train_data.to_numpy())
train_label_np = copy.copy(train_label.to_numpy())
        
train_data_np = preprocessing.normalize(train_data_np,axis=0)        
      
# train nms
svm_model = svm_func(train_data_np,train_label_np,'sigmoid')

test_data_np = copy.copy(test_data.to_numpy())
test_label_np = copy.copy(test_label.to_numpy())
        
test_data_np = preprocessing.normalize(test_data_np,axis=0)

model_test_acc(svm_model,test_data_np,test_label_np)

train_data_np = copy.copy(train_data_augmented.to_numpy())
train_label_np = copy.copy(train_label_augmented.to_numpy())
        
train_data_np = preprocessing.normalize(train_data_np,axis=0)        
      
# train nms
svm_model = svm_func(train_data_np,train_label_np,'linear')

test_data_np = copy.copy(test_data_augmented.to_numpy())
test_label_np = copy.copy(test_label_augmented.to_numpy())
        
test_data_np = preprocessing.normalize(test_data_np,axis=0)

model_test_acc(svm_model,test_data_np,test_label_np)

"""### PCA + SVM"""

# We select the best number of components by Cross-validation
kf = KFold(n_splits=7)
best_val_acc = 0

n_components = [3,4,5,6,7,8,9]
kernel_type = ['linear','rbf','sigmoid']
for k in kernel_type:
    for n in n_components:
        val_acc=0
        for train_index,val_index in kf.split(train_data_augmented):
        
            train_data_np = copy.copy(train_data_augmented.iloc[train_index,:].to_numpy())
            train_label_np = copy.copy(train_label_augmented.iloc[train_index].to_numpy())
        
            val_data_np = copy.copy(train_data_augmented.iloc[val_index,:].to_numpy())
            val_label_np = copy.copy(train_label_augmented.iloc[val_index].to_numpy())
        
            pca = PCA(n_components=n)
        
            train_data_pca_np = pca.fit_transform(train_data_np)
            val_data_pca_np = pca.transform(val_data_np)
        
        # train nms
            svm_pca_func = svm_func(train_data_pca_np,train_label_np,k,False)
        #get val acc
            acc = model_test_acc(svm_pca_func,val_data_pca_np,val_label_np,False)
            val_acc = val_acc + acc
        val_acc = val_acc/7
        if(val_acc >= best_val_acc):
            best_val_acc = val_acc
            n_best = n
            k_best = k
            print(f'For kernel = {k} and n = {n}, validation accuracy is {val_acc}')
print(f'Best performance for kernel = {k_best} and n = {n_best}, validation accuracy is {best_val_acc}')

train_data_np = copy.copy(train_data_augmented.to_numpy())
train_label_np = copy.copy(train_label_augmented.to_numpy())
        

pca = PCA(n_components=6)
        
train_data_pca_np = pca.fit_transform(train_data_np)
        
# train nms
svm_pca_func = svm_func(train_data_pca_np,train_label_np,'linear')

test_data_np = copy.copy(test_data_augmented.to_numpy())
test_label_np = copy.copy(test_label_augmented.to_numpy())
        
test_data_pca_np = pca.transform(test_data_np)

model_test_acc(svm_pca_func,test_data_pca_np,test_label_np)

"""## PCA + K-nearest Neighbours"""

def knn_func(data,labels,show=True):
    train_data_np = data
    train_labels_np = labels
    
    knn_model = sklearn.neighbors.KNeighborsClassifier(n_neighbors=3)
    knn_model.fit(train_data_np,train_labels_np)

    train_pred_nms = np.zeros([len(train_data_np)])
    for i in range(len(train_data_np)):
        train_pred_nms[i] = knn_model.predict([train_data_np[i,:]])

    score_metrics(train_labels_np,train_pred_nms,show)
    return knn_model # we return the trained model

train_data_np = copy.copy(train_data_augmented.to_numpy())
train_label_np = copy.copy(train_label_augmented.to_numpy())

train_data_np = preprocessing.normalize(train_data_np,axis=0)

pca = PCA(n_components=6)
        
train_data_pca_np = pca.fit_transform(train_data_np)
        
# train nms
knn_pca_func = knn_func(train_data_pca_np,train_label_np)

test_data_np = copy.copy(test_data_augmented.to_numpy())
test_label_np = copy.copy(test_label_augmented.to_numpy())

test_data_np = preprocessing.normalize(test_data_np,axis=0)

test_data_pca_np = pca.transform(test_data_np)

model_test_acc(knn_pca_func,test_data_pca_np,test_label_np)

"""# Decision Tree"""

def decision_tree_func(data,labels,depth,show=True):
    train_data_np = data
    train_labels_np = labels
    
    d_tree_model = sklearn.tree.DecisionTreeClassifier(max_depth=depth)
    d_tree_model.fit(train_data_np,train_labels_np)

    train_pred_nms = np.zeros([len(train_data_np)])
    for i in range(len(train_data_np)):
        train_pred_nms[i] = d_tree_model.predict([train_data_np[i,:]])

    score_metrics(train_labels_np,train_pred_nms,show)
    return d_tree_model # we return the trained model

train_data_np = copy.copy(train_data_augmented.to_numpy())
train_label_np = copy.copy(train_label_augmented.to_numpy())

train_data_np = preprocessing.normalize(train_data_np,axis=0)
        
# train nms
dtree_func = decision_tree_func(train_data_np,train_label_np,3)

test_data_np = copy.copy(test_data_augmented.to_numpy())
test_label_np = copy.copy(test_label_augmented.to_numpy())

test_data_np = preprocessing.normalize(test_data_np,axis=0)

model_test_acc(dtree_func,test_data_np,test_label_np)

"""# Random Forest"""

def random_forest_func(data,labels,depth,show=True):
    train_data_np = data
    train_labels_np = labels
    
    r_forest_model = sklearn.ensemble.RandomForestClassifier(max_depth=depth,random_state=0)
    r_forest_model.fit(train_data_np,train_labels_np)

    train_pred_nms = np.zeros([len(train_data_np)])
    for i in range(len(train_data_np)):
        train_pred_nms[i] = r_forest_model.predict([train_data_np[i,:]])

    score_metrics(train_labels_np,train_pred_nms,show)
    return r_forest_model # we return the trained model

train_data_np = copy.copy(train_data_augmented.to_numpy())
train_label_np = copy.copy(train_label_augmented.to_numpy())

train_data_np = preprocessing.normalize(train_data_np,axis=0)
        
# train nms
rforest_func = decision_tree_func(train_data_np,train_label_np,2)

test_data_np = copy.copy(test_data_augmented.to_numpy())
test_label_np = copy.copy(test_label_augmented.to_numpy())

test_data_np = preprocessing.normalize(test_data_np,axis=0)

model_test_acc(rforest_func,test_data_np,test_label_np)

"""# Naive Bayes """

def naive_bayes_func(data,labels,show=True):
    train_data_np = data
    train_labels_np = labels
    
    nb_model = sklearn.naive_bayes.GaussianNB()
    nb_model.fit(train_data_np,train_labels_np)

    train_pred_nms = np.zeros([len(train_data_np)])
    for i in range(len(train_data_np)):
        train_pred_nms[i] = nb_model.predict([train_data_np[i,:]])

    score_metrics(train_labels_np,train_pred_nms,show)
    return nb_model # we return the trained model

train_data_np = copy.copy(train_data_augmented.to_numpy())
train_label_np = copy.copy(train_label_augmented.to_numpy())

train_data_np = preprocessing.normalize(train_data_np,axis=0)
        
# train nms
nb_func = naive_bayes_func(train_data_np,train_label_np,2)

test_data_np = copy.copy(test_data_augmented.to_numpy())
test_label_np = copy.copy(test_label_augmented.to_numpy())

test_data_np = preprocessing.normalize(test_data_np,axis=0)

model_test_acc(nb_func,test_data_np,test_label_np)



